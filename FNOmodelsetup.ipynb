{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb37b1fa",
   "metadata": {},
   "source": [
    "Sets up dataloader for FDTD data from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a9a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "\n",
    "DATA_FILE = \"fdtdData_random_smooth_nonlinear_SET1.npz\"\n",
    "#Load Data\n",
    "data = np.load(DATA_FILE, allow_pickle=True)\n",
    "samples = data[\"samples\"]\n",
    "sample_count = 1000\n",
    "samples = samples[:sample_count]\n",
    "prediction_index = 8\n",
    "\n",
    "all_inputs = []\n",
    "all_outputs = []\n",
    "\n",
    "# Compute means and stds for normalization \n",
    "# Inputs\n",
    "epsR_all = np.concatenate([s[\"epsR\"] for s in samples])\n",
    "src0_all = np.concatenate([s[\"source_prop\"][0] for s in samples])\n",
    "src1_all = np.concatenate([s[\"source_prop\"][1] for s in samples])\n",
    "\n",
    "epsR_mean, epsR_std = epsR_all.mean(), epsR_all.std()\n",
    "src0_mean, src0_std = src0_all.mean(), src0_all.std()\n",
    "src1_mean, src1_std = src1_all.mean(), src1_all.std()\n",
    "\n",
    "# Outputs\n",
    "ez_all = np.concatenate([s[\"ez_history\"][prediction_index] for s in samples])\n",
    "hy_all = np.concatenate([s[\"hy_history\"][prediction_index] for s in samples])\n",
    "\n",
    "ez_mean, ez_std = ez_all.mean(), ez_all.std()\n",
    "hy_mean, hy_std = hy_all.mean(), hy_all.std()\n",
    "\n",
    "for sample in samples:\n",
    "    N = sample[\"epsR\"].shape[0]\n",
    "    \n",
    "\n",
    "    ez_pred_snaps = np.array(sample[\"ez_history\"])[prediction_index]\n",
    "    hy_pred_snaps = np.array(sample[\"hy_history\"])[prediction_index]\n",
    "    pred_time_points = np.array(sample[\"time_points\"])[prediction_index]\n",
    "    # Normalize inputs\n",
    "    epsR_norm = (sample[\"epsR\"] - epsR_mean) / epsR_std\n",
    "    src0_norm = (sample[\"source_prop\"][0] - src0_mean) / src0_std\n",
    "    src1_norm = (sample[\"source_prop\"][1] - src1_mean) / src1_std\n",
    "        \n",
    "    # Stack input features \n",
    "    sample_inputs = np.stack([\n",
    "        epsR_norm,\n",
    "        src0_norm,\n",
    "        src1_norm\n",
    "    ], axis=0)  \n",
    "\n",
    "    all_inputs.append(sample_inputs)\n",
    "\n",
    "    # Normalize outputs\n",
    "    ez_norm = (ez_pred_snaps - ez_mean) / ez_std\n",
    "    hy_norm = (hy_pred_snaps - hy_mean) / hy_std\n",
    "        \n",
    "    #Stack outputs (shape = (2, ...))\n",
    "    sample_outputs = np.stack((ez_norm, hy_norm), axis=0)\n",
    "    all_outputs.append(sample_outputs)\n",
    "\n",
    "# Convert lists to arrays\n",
    "all_inputs = np.stack(all_inputs, axis=0)   \n",
    "\n",
    "all_outputs = np.stack(all_outputs, axis=0) \n",
    "\n",
    "class FieldDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        inputs: numpy array, shape (N, C_in, L)\n",
    "        outputs: numpy array, shape (N, C_out, L)\n",
    "        \"\"\"\n",
    "        self.inputs = torch.from_numpy(inputs).float()\n",
    "        self.outputs = torch.from_numpy(outputs).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "    \n",
    "dataset = FieldDataset(all_inputs, all_outputs)\n",
    "        \n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614170c",
   "metadata": {},
   "source": [
    "Fourier layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e369767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mask):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, multiplies selected spectral modes, and Inverse FFT.\n",
    "        mask: boolean tensor indicating which Fourier modes to keep (length = N//2+1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.mask = mask               # boolean mask for Fourier modes\n",
    "        self.num = torch.sum(mask)     # number of modes to keep\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        # Complex weights for selected modes\n",
    "        self.weights = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.num, dtype=torch.cfloat))\n",
    "\n",
    "        # Optional local 1D convolution for residual / smoothing in time domain\n",
    "        self.w = nn.Conv1d(in_channels, in_channels, kernel_size=3, padding=1, bias=True)\n",
    "\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # input: (B, c, num_modes), weights: (c_in, c_out, num_modes)\n",
    "        # output: (B, c_out, num_modes)\n",
    "        return torch.einsum(\"bcn,con->bon\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize, c, N = x.shape\n",
    "        x_ft = torch.fft.rfft(x, dim=-1)\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x_ft.shape[-1], dtype=torch.cfloat, device=x.device)\n",
    "        mask_indices = self.mask.nonzero(as_tuple=True)[0]\n",
    "        x_selected = x_ft[:, :, mask_indices]\n",
    "        out_ft[:, :, mask_indices] = self.compl_mul1d(x_selected, self.weights)\n",
    "        x = torch.fft.irfft(out_ft, n=N, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc9b19",
   "metadata": {},
   "source": [
    "Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53c038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Learnable1DPosEmbedding(nn.Module):\n",
    "    def __init__(self, N, num_freqs):\n",
    "        super().__init__()\n",
    "        # Learnable embedding for N points, 2*num_freqs channels\n",
    "        self.embedding = nn.Parameter(torch.randn(2*num_freqs, N))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C_in, N)\n",
    "        B = x.shape[0]\n",
    "        emb = self.embedding.unsqueeze(0).repeat(B, 1, 1)  # (B, 2*num_freqs, N)\n",
    "        return torch.cat([x, emb], dim=1)\n",
    "\n",
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, width, layers, mask, N, padding=0,\n",
    "                 num_pos_freqs=4, use_learnable_pe=True, use_channel_mlp=True):\n",
    "        \"\"\"\n",
    "        1D Fourier Neural Operator inspired model.\n",
    "\n",
    "        Args:\n",
    "            in_channels: input channels (e.g. epsR, source_prop)\n",
    "            out_channels: output channels (e.g. Ez, Hy)\n",
    "            width: hidden width for latent space\n",
    "            layers: number of Fourier layers\n",
    "            mask: boolean tensor for allowed Fourier modes (length=N//2+1)\n",
    "            padding: optional padding\n",
    "            num_pos_freqs: number of learnable positional frequencies\n",
    "            use_learnable_pe: whether to add learnable positional embeddings\n",
    "            use_channel_mlp: whether to add a channel-wise MLP per Fourier layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.padding = padding\n",
    "        self.mask = mask\n",
    "        self.use_channel_mlp = use_channel_mlp\n",
    "\n",
    "        # Optional learnable positional embeddings\n",
    "        self.pos_embedding = Learnable1DPosEmbedding(N, num_pos_freqs) if use_learnable_pe else None\n",
    "\n",
    "        # Input lifting to latent space\n",
    "        lift_in_channels = in_channels + (2*num_pos_freqs if use_learnable_pe else 0)\n",
    "        self.lifting = nn.Sequential(\n",
    "            nn.Linear(lift_in_channels, width),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width, width)\n",
    "        )\n",
    "\n",
    "        # Fourier layers + residuals + optional channel MLP\n",
    "        self.conv_layers = nn.ModuleList([SpectralConv1d(width, width, mask) for _ in range(layers)])\n",
    "        self.res_layers = nn.ModuleList([nn.Conv1d(width, width, kernel_size=1) for _ in range(layers)])\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(width) for _ in range(layers)])\n",
    "\n",
    "        if use_channel_mlp:\n",
    "            self.channel_mlps = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(width, width*2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(width*2, width)\n",
    "                ) for _ in range(layers)\n",
    "            ])\n",
    "        else:\n",
    "            self.channel_mlps = [None]*layers\n",
    "\n",
    "        # Output projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(width, width*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(width*2, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, C_in, N)\n",
    "        returns: (B, out_channels, N)\n",
    "        \"\"\"\n",
    "        # Add positional embedding\n",
    "        if self.pos_embedding is not None:\n",
    "            x = self.pos_embedding(x)  # (B, C_in + PE, N)\n",
    "\n",
    "        # Lift to latent space\n",
    "        x = x.permute(0, 2, 1)  # (B, N, C)\n",
    "        x = self.lifting(x)     # (B, N, width)\n",
    "        x = x.permute(0, 2, 1)  # (B, width, N)\n",
    "\n",
    "        # Optional padding\n",
    "        if self.padding > 0:\n",
    "            x = F.pad(x, (0, self.padding))\n",
    "\n",
    "        # Fourier layers\n",
    "        for i, (conv, res, bn) in enumerate(zip(self.conv_layers, self.res_layers, self.bn_layers)):\n",
    "            x1 = conv(x)\n",
    "            x2 = res(x)\n",
    "            x = x1 + x2\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            # Optional channel MLP\n",
    "            if self.use_channel_mlp and self.channel_mlps[i] is not None:\n",
    "                x_perm = x.permute(0, 2, 1)  # (B, N, width)\n",
    "                x_perm = self.channel_mlps[i](x_perm)\n",
    "                x = x_perm.permute(0, 2, 1)\n",
    "\n",
    "        # Remove padding\n",
    "        if self.padding > 0:\n",
    "            x = x[:, :, :-self.padding]\n",
    "\n",
    "        # Project to output\n",
    "        x = x.permute(0, 2, 1)  # (B, N, width)\n",
    "        x = self.projection(x)  # (B, N, out_channels)\n",
    "        x = x.permute(0, 2, 1)  # (B, out_channels, N)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c5ded4",
   "metadata": {},
   "source": [
    "New mode selection scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32830f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def compute_spectral_mask(all_outputs, theta=0.9):\n",
    "    \"\"\"\n",
    "    Compute a data-driven spectral mask for 1D FNO similar to Wu paper.\n",
    "    all_outputs: np.array, shape (num_samples, out_channels, N)\n",
    "    theta: cumulative energy threshold (0-1)\n",
    "    Returns: mask (torch.bool, shape (N//2+1,))\n",
    "    \"\"\"\n",
    "    # FFT along spatial dimension\n",
    "    spectra = np.fft.rfft(all_outputs, axis=-1)\n",
    "    magnitudes = np.abs(spectra)\n",
    "    \n",
    "    # Average over all samples and channels\n",
    "    mean_spectrum = magnitudes.mean(axis=(0,1))\n",
    "    \n",
    "    # Normalize cumulative sum\n",
    "    cumulative = np.cumsum(mean_spectrum) / np.sum(mean_spectrum)\n",
    "    \n",
    "    # Keep modes where cumulative < theta\n",
    "    mask = cumulative <= theta\n",
    "    return torch.tensor(mask, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d57bb8",
   "metadata": {},
   "source": [
    "initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc12fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "mask = compute_spectral_mask(all_outputs, theta=0.4)\n",
    "num_modes_kept = mask.sum().item()  # convert tensor to integer\n",
    "print(\"Number of Fourier modes kept:\", num_modes_kept)\n",
    "in_channels = all_inputs.shape[1]   \n",
    "out_channels = all_outputs.shape[1] \n",
    "width = 15\n",
    "layers = 5\n",
    "padding = int(0.05 * N)  # optional\n",
    "\n",
    "model = FNO1d(in_channels, out_channels, width, layers, mask, N=all_inputs.shape[-1], padding=padding).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c994274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Relative L2 loss function. just MSE\n",
    "class RelativeL2Loss1D(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super(RelativeL2Loss1D, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        y_pred, y_true: (B, C, N)\n",
    "        \"\"\"\n",
    "        diff = y_pred - y_true\n",
    "        numerator = torch.linalg.norm(diff, dim=(1,2))          # L2 over channels and sequence\n",
    "        denominator = torch.linalg.norm(y_true, dim=(1,2)) + self.eps\n",
    "        relative_l2 = numerator / denominator\n",
    "        return relative_l2.mean()\n",
    "\n",
    "criterion = RelativeL2Loss1D()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10\n",
    ")\n",
    "\n",
    "epochs = 200\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Training\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # stabilize training\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    \n",
    "    # Validation\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} | \"\n",
    "        f\"Train RelL2: {train_loss:.6f} | \"\n",
    "        f\"Val RelL2: {val_loss:.6f} | \"\n",
    "        f\"LR: {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "    )\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Relative L2 Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbf6641",
   "metadata": {},
   "source": [
    "save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab41b8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\".\\checkpoints\\model_{sample_count}samples_1step_linear_smooth_extralayersrun.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d54fc7c",
   "metadata": {},
   "source": [
    "check a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc86d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Pick a batch from validation set\n",
    "x_batch, y_batch = next(iter(val_loader))\n",
    "x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_batch)\n",
    "\n",
    "# Move back to CPU for plotting\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "y_batch = y_batch.cpu().numpy()\n",
    "x_batchp = x_batch.cpu().numpy()\n",
    "\n",
    "# Pick the first sample in the batch\n",
    "sample_idx = 0\n",
    "\n",
    "# Assume channel 0 is Ez\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y_batch[sample_idx, 0], label='True Ez', color='blue')\n",
    "plt.plot(y_pred[sample_idx, 0], label='Predicted Ez', color='red', linestyle='--')\n",
    "#plt.plot(x_batchp[sample_idx, 0], label = 'eps', color = 'green') #This plots eps optionally\n",
    "#plt.plot(x_batchp[sample_idx, 1], label = 'source', color = 'purple') #this plots the source\n",
    "plt.xlabel('Grid Point Index')\n",
    "plt.ylabel('Ez Field')\n",
    "plt.title('Model Prediction vs True Ez')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e405293",
   "metadata": {},
   "source": [
    "plot more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(min(10, x_batch.size(0))):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(y_batch[i,0], label='True Ez')\n",
    "    plt.plot(y_pred[i,0], label='Predicted Ez', linestyle='--')\n",
    "    #plt.plot(x_batchp[i, 0], label = 'eps', color = 'grey')\n",
    "    #plt.plot(x_batchp[i, 1], label = 'source', color = 'purple')\n",
    "    plt.title(f'Sample {i}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
